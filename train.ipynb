{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from emoDB import EmodbDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import cuda\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emodb_collate_fn(batch):\n",
    "    waveforms = [item['waveform'] for item in batch]\n",
    "    sample_rates = [item['sample_rate'] for item in batch]\n",
    "    emotions = [item['emotion'] for item in batch]\n",
    "\n",
    "    # Find max length in waveforms\n",
    "    max_length = max([waveform.size(1) for waveform in waveforms])\n",
    "\n",
    "    # Pad all waveforms to max_length\n",
    "    waveforms_padded = []\n",
    "    for waveform in waveforms:\n",
    "        pad_amount = max_length - waveform.size(1)\n",
    "        waveform_padded = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
    "        waveforms_padded.append(waveform_padded)\n",
    "    # Stack everything up\n",
    "    \n",
    "    waveforms_padded = torch.stack(waveforms_padded)\n",
    "    sample_rates = torch.stack([torch.tensor(sr) for sr in sample_rates])\n",
    "    emotions = torch.stack([torch.tensor(em) for em in emotions])\n",
    "\n",
    "    # Create attention mask\n",
    "    #attention_masks_padded = torch.where(waveforms_padded != 0, 1, 0)\n",
    "    attention_masks_padded = (waveforms_padded != 0)\n",
    "    return waveforms_padded, sample_rates, emotions, attention_masks_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\UCLA\\cs260\\realproject\\emoDB.py:36: FutureWarning: Could not cast to float32, falling back to object. This behavior is deprecated. In a future version, when a dtype is passed to 'DataFrame', either all columns will be cast to that dtype, or a TypeError will be raised.\n",
      "  self.df = pd.DataFrame(data, columns=['speaker_id', 'code', 'emotion', 'version', 'file'], dtype=np.float32)\n"
     ]
    }
   ],
   "source": [
    "emoDB = EmodbDataset('./emoDB/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Let's say you want to use 80% of the data for training, and 20% for testing\n",
    "train_size = int(0.8 * len(emoDB))\n",
    "test_size = len(emoDB) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(emoDB, [train_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=emodb_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=emodb_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  ..., False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  ..., False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  ..., False, False, False]]])\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_loader))\n",
    "print(sample[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dl):\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for X, rate, y, attention_mask in test_dl:\n",
    "      X, y, attention_mask = X.to(device), y.to(device), attention_mask.to(device)\n",
    "      outputs = model.forward(X)\n",
    "      max, preds = torch.max(outputs.data,1)\n",
    "      total += y.size(0)\n",
    "      #print(preds)\n",
    "      #print(y)\n",
    "      correct += (preds == y).sum().item()\n",
    "  accuracy = correct / total\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr, num_epochs, train_dl, test_dl):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X, rate, y, attention_mask in train_dl:\n",
    "            #print(y.shape)\n",
    "            #y = y.long()\n",
    "            X, y, attention_mask = X.to(device), y.to(device), attention_mask.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #print(y.shape)\n",
    "            #print(X.shape)\n",
    "            #print(attention_mask.shape)\n",
    "            outputs = model(X)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, y)\n",
    "            #print()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            \"\"\"\n",
    "            for name, parameter in model.named_parameters():\n",
    "                if parameter.grad is not None:\n",
    "                    print(f'{name}: gradient max {parameter.grad.data.abs().max()}, gradient min {parameter.grad.data.abs().min()}')\n",
    "            \"\"\"\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            #print(running_loss)\n",
    "            _, preds = outputs.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += (preds == y).sum().item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss: {running_loss / len(train_loader)} | Accuracy: {100.*correct/total}\")\n",
    "        #test_accuracy = test(model,test_dl)\n",
    "        #print(f\"Test Accuracy: {test_accuracy}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMO_CLASSES = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.layers.7.attention.out_proj.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.11.attention.k_proj.weight', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.6.attention.k_proj.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.6.layer_norm.bias', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.9.layer_norm.weight', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.10.attention.k_proj.bias', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.6.attention.q_proj.weight', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.6.attention.k_proj.bias']\n",
      "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1 | Loss: 1.5814092407159717 | Accuracy: 37.149532710280376\n",
      "Test Accuracy: 0.6074766355140186\n",
      "Epoch 2 | Loss: 1.170530772097757 | Accuracy: 53.97196261682243\n",
      "Test Accuracy: 0.5420560747663551\n",
      "Epoch 3 | Loss: 0.9197530951176849 | Accuracy: 65.18691588785046\n",
      "Test Accuracy: 0.4672897196261682\n",
      "Epoch 4 | Loss: 0.7689779881283502 | Accuracy: 73.13084112149532\n",
      "Test Accuracy: 0.8037383177570093\n",
      "Epoch 5 | Loss: 0.5844865975048498 | Accuracy: 79.67289719626169\n",
      "Test Accuracy: 0.8130841121495327\n",
      "Epoch 6 | Loss: 0.41631464299824195 | Accuracy: 87.14953271028037\n",
      "Test Accuracy: 0.8598130841121495\n",
      "Epoch 7 | Loss: 0.3090216882579098 | Accuracy: 91.82242990654206\n",
      "Test Accuracy: 0.8598130841121495\n",
      "Epoch 8 | Loss: 0.22858841393021084 | Accuracy: 94.1588785046729\n",
      "Test Accuracy: 0.8037383177570093\n",
      "Epoch 9 | Loss: 0.18205598725232286 | Accuracy: 95.32710280373831\n",
      "Test Accuracy: 0.8691588785046729\n",
      "Epoch 10 | Loss: 0.07247004911703901 | Accuracy: 98.59813084112149\n",
      "Test Accuracy: 0.8411214953271028\n",
      "Epoch 11 | Loss: 0.20827162910305033 | Accuracy: 94.39252336448598\n",
      "Test Accuracy: 0.7757009345794392\n",
      "Epoch 12 | Loss: 0.2091355867940654 | Accuracy: 95.32710280373831\n",
      "Test Accuracy: 0.7289719626168224\n",
      "Epoch 13 | Loss: 0.1267169692029167 | Accuracy: 97.19626168224299\n",
      "Test Accuracy: 0.8691588785046729\n",
      "Epoch 14 | Loss: 0.19768498646243363 | Accuracy: 96.49532710280374\n",
      "Test Accuracy: 0.9158878504672897\n",
      "Epoch 15 | Loss: 0.1653499808652048 | Accuracy: 96.49532710280374\n",
      "Test Accuracy: 0.8411214953271028\n",
      "Epoch 16 | Loss: 0.08343328450910886 | Accuracy: 98.59813084112149\n",
      "Test Accuracy: 0.9158878504672897\n",
      "Epoch 17 | Loss: 0.13750840861472152 | Accuracy: 96.26168224299066\n",
      "Test Accuracy: 0.8691588785046729\n",
      "Epoch 18 | Loss: 0.026171067752984188 | Accuracy: 99.53271028037383\n",
      "Test Accuracy: 0.7757009345794392\n",
      "Epoch 19 | Loss: 0.181270641315006 | Accuracy: 95.79439252336448\n",
      "Test Accuracy: 0.8598130841121495\n",
      "Epoch 20 | Loss: 0.18878123887257528 | Accuracy: 96.96261682242991\n",
      "Test Accuracy: 0.8785046728971962\n",
      "Epoch 21 | Loss: 0.06745427856482913 | Accuracy: 98.83177570093459\n",
      "Test Accuracy: 0.794392523364486\n",
      "Epoch 22 | Loss: 0.11389221487758315 | Accuracy: 97.66355140186916\n",
      "Test Accuracy: 0.8504672897196262\n",
      "Epoch 23 | Loss: 0.34084830773509084 | Accuracy: 92.5233644859813\n",
      "Test Accuracy: 0.794392523364486\n",
      "Epoch 24 | Loss: 0.09820339465502946 | Accuracy: 97.66355140186916\n",
      "Test Accuracy: 0.8691588785046729\n",
      "Epoch 25 | Loss: 0.0666970991068215 | Accuracy: 98.59813084112149\n",
      "Test Accuracy: 0.8317757009345794\n",
      "Epoch 26 | Loss: 0.1397893552953447 | Accuracy: 97.66355140186916\n",
      "Test Accuracy: 0.7850467289719626\n",
      "Epoch 27 | Loss: 0.20847770969261564 | Accuracy: 96.49532710280374\n",
      "Test Accuracy: 0.8317757009345794\n",
      "Epoch 28 | Loss: 0.22142207210613676 | Accuracy: 95.79439252336448\n",
      "Test Accuracy: 0.7663551401869159\n",
      "Epoch 29 | Loss: 0.23668009967044884 | Accuracy: 95.09345794392523\n",
      "Test Accuracy: 0.8691588785046729\n",
      "Epoch 30 | Loss: 0.18397416526323324 | Accuracy: 96.49532710280374\n",
      "Test Accuracy: 0.8411214953271028\n",
      "final accuracy = 0.12149532710280374\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "import importlib\n",
    "importlib.reload(models)\n",
    "from models import Baseline\n",
    "from models import TransferModel\n",
    "from models import ScratchModel\n",
    "baseline_model = Baseline(EMO_CLASSES)\n",
    "transfer_model = TransferModel(EMO_CLASSES)\n",
    "scratch_model = ScratchModel(EMO_CLASSES)\n",
    "lr = 0.0001\n",
    "num_epochs = 30\n",
    "train(transfer_model, lr, num_epochs, train_loader, test_loader)\n",
    "final_acc = test(transfer_model, test_loader)\n",
    "print(f\"final accuracy = {final_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.93k/1.93k [00:00<00:00, 644kB/s]\n",
      "c:\\Users\\randy\\miniconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\randy\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading pytorch_model.bin: 100%|██████████| 378M/378M [00:37<00:00, 10.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import HubertForSequenceClassification\n",
    "hubert_model = HubertForSequenceClassification.from_pretrained(\"superb/hubert-base-superb-ks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
